{
  "_comment": "Copy this file to .env or set as environment variables. Do NOT commit secrets.",
  "_note": "Set exactly ONE of the three LLM backend sections below.",

  "_option1_azure_openai": {
    "AZURE_OPENAI_ENDPOINT": "https://<your-resource>.openai.azure.com/",
    "AZURE_OPENAI_API_KEY": "<your-azure-openai-api-key>",
    "AZURE_OPENAI_DEPLOYMENT": "gpt-4o"
  },

  "_option2_any_openai_compatible_endpoint": {
    "_examples": [
      "Ollama (local):    http://localhost:11434/v1",
      "llama.cpp server:  http://localhost:8080/v1",
      "LM Studio:         http://localhost:1234/v1",
      "Any remote proxy:  https://my-proxy.example.com/v1"
    ],
    "OPENAI_BASE_URL": "http://localhost:11434/v1",
    "OPENAI_MODEL": "llama3",
    "_note": "OPENAI_API_KEY is optional for local servers that don't validate the key. Set it to the expected value if your server requires authentication."
  },

  "_option3_standard_openai": {
    "OPENAI_API_KEY": "sk-...",
    "OPENAI_MODEL": "gpt-4o"
  },

  "MEMORY_FILE_PATH": "memory.json"
}
